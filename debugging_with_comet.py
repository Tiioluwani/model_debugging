# -*- coding: utf-8 -*-
"""debugging_with_comet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/Tiioluwani/cbff2cca16306d4df61f085f8f1919e5/debugging_with_comet.ipynb
"""

!pip install scikit-optimize
!pip install comet_ml

import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_curve, auc
from sklearn.preprocessing import label_binarize
from skopt import BayesSearchCV
from comet_ml import Experiment
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

def generate_data():
    # Generate synthetic dataset
    X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
    return X, y

def split_data(X, y):
    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    return X_train, X_test, y_train, y_test

def optimize_hyperparameters(X_train, y_train, experiment):
    # Optimize hyperparameters using Bayesian optimization
    param_space = {
        'n_estimators': (50, 150),
        'max_depth': (2, 20),
        'min_samples_split': (2, 10),
        'min_samples_leaf': (1, 10)
    }

    opt = BayesSearchCV(
        RandomForestClassifier(),
        param_space,
        n_iter=20,
        cv=5,
        n_jobs=-1
    )

    opt.fit(X_train, y_train)
    best_params = opt.best_params_
    # Log optimized hyperparameters to Comet.ml
    experiment.log_parameters(best_params)

    return best_params

from sklearn.preprocessing import label_binarize

def plot_roc_curve(model, X_test, y_test, experiment, model_name):
    # Ensure that y_test is binary
    y_test_binarized = label_binarize(y_test, classes=[0, 1])[:, 1]

    # Calculate probabilities for the positive class
    y_scores = model.predict_proba(X_test)

    # Check if the output has the correct shape
    if y_scores.shape[1] != 2:
        raise ValueError(f"Expected 2 columns in y_scores, but got {y_scores.shape[1]}")

    # Use the scores for the positive class
    y_scores_positive = y_scores[:, 1]

    # Compute ROC curve and area under the curve
    fpr, tpr, _ = roc_curve(y_test_binarized, y_scores_positive)
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {model_name}')
    plt.legend(loc="lower right")

    # Save and log ROC curve to Comet.ml
    roc_curve_path = f"{model_name}_roc_curve.png"
    plt.savefig(roc_curve_path)
    experiment.log_asset(roc_curve_path, file_name=roc_curve_path)

    plt.close()

def log_confusion_matrix(model, X_test, y_test, experiment, model_name):
    # Log confusion matrix to Comet.ml
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    experiment.log_confusion_matrix(y_test, y_pred, title=f'Confusion Matrix - {model_name}')

def log_feature_importance(model, X_train, experiment):
    # Log feature importance for Random Forest model
    if isinstance(model, RandomForestClassifier):
        feature_importance = model.feature_importances_
        features = [f"Feature {i+1}" for i in range(len(feature_importance))]

        plt.figure(figsize=(10, 6))
        plt.bar(features, feature_importance)
        plt.title('Feature Importance - Random Forest')
        plt.xlabel('Features')
        plt.ylabel('Importance')

        # Save and log feature importance plot to Comet.ml
        feature_importance_path = "random_forest_feature_importance.png"
        plt.savefig(feature_importance_path)
        experiment.log_asset(feature_importance_path, file_name=feature_importance_path)

        plt.close()

def log_neural_network_architecture(model, experiment):
    # Log architecture for Neural Network model
    if isinstance(model, Sequential):
        architecture_summary = []
        model.summary(print_fn=lambda x: architecture_summary.append(x))
        architecture_summary = "\n".join(architecture_summary)

        # Log architecture summary to Comet.ml
        experiment.log_text("Neural Network Architecture Summary", architecture_summary)

def log_training_history(history, experiment, model_name):
    # Log training history to Comet.ml
    for metric in history.history:
        experiment.log_metric(f"{model_name}_{metric}", history.history[metric])

def train_rf_model(X_train, y_train, hyperparameters):
    # Train the Random Forest model with optimized hyperparameters
    model = RandomForestClassifier(**hyperparameters, random_state=42)
    model.fit(X_train, y_train)
    return model

def train_nn_model(X_train, y_train):
    # Train a simple Neural Network model
    model = Sequential()
    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Train the model and log training history
    history = model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)

    return model, history

def main():
    # Initialize Comet.ml experiment
    experiment = Experiment(api_key="", project_name="model-debugging")

    # Generate synthetic data
    X, y = generate_data()

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = split_data(X, y)

    # A/B Testing: Train Random Forest model
    rf_hyperparameters = optimize_hyperparameters(X_train, y_train, experiment)
    rf_model = train_rf_model(X_train, y_train, rf_hyperparameters)

    # A/B Testing: Train Neural Network model
    nn_model, nn_history = train_nn_model(X_train, y_train)

    # Log confusion matrix, ROC curve, feature importance, and architecture
    log_confusion_matrix(rf_model, X_test, y_test, experiment, "random_forest")
    plot_roc_curve(rf_model, X_test, y_test, experiment, "random_forest")
    log_feature_importance(rf_model, X_train, experiment)

    log_confusion_matrix(nn_model, X_test, y_test, experiment, "neural_network")
    plot_roc_curve(nn_model, X_test, y_test, experiment, "neural_network")
    log_neural_network_architecture(nn_model, experiment)

    # Log training history for both models
    log_training_history(nn_history, experiment, "neural_network")

if __name__ == "__main__":
    main()